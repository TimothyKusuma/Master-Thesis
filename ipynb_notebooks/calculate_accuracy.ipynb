{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import glob\n",
    "import Levenshtein as lev\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yaml(file_path):\n",
    "    with open(file_path, 'r') as stream:\n",
    "        try:\n",
    "            return yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_constructs(true_constructs, detected_constructs, max_distance=0):\n",
    "    true_set = set(true_constructs.values())\n",
    "    detected_set = set(detected_constructs.values())\n",
    "    TP = sum(1 for det in detected_set if any(is_similar(det, tru, max_distance) for tru in true_set))\n",
    "    FP = len(detected_set) - TP\n",
    "    FN = len(true_set) - TP\n",
    "    return TP, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_hypotheses(true_constructs, detected_constructs, true_hypotheses, detected_hypotheses):\n",
    "    # Translate hypothesis keys to construct names for true data\n",
    "    true_hypotheses_translated = {(true_constructs[h['cause']], true_constructs[h['effect']]) for h in true_hypotheses.values()}\n",
    "\n",
    "    # Translate hypothesis keys to construct names for detected data\n",
    "    detected_hypotheses_translated = set()\n",
    "    for h in detected_hypotheses.values():\n",
    "        cause = h.get('cause')\n",
    "        effect = h.get('effect')\n",
    "        if cause in detected_constructs and effect in detected_constructs:\n",
    "            detected_hypotheses_translated.add((detected_constructs[cause], detected_constructs[effect]))\n",
    "\n",
    "    TP = len(true_hypotheses_translated.intersection(detected_hypotheses_translated))\n",
    "    FP = len(detected_hypotheses_translated - true_hypotheses_translated)\n",
    "    FN = len(true_hypotheses_translated - detected_hypotheses_translated)\n",
    "\n",
    "    # Evaluate label correctness for TP hypotheses\n",
    "    correct_labels_count = 0\n",
    "    for h in true_hypotheses.values():\n",
    "        if (true_constructs[h['cause']], true_constructs[h['effect']]) in detected_hypotheses_translated:\n",
    "            detected_hypothesis = next((dh for dh in detected_hypotheses.values() if dh['cause'] == h['cause'] and dh['effect'] == h['effect']), None)\n",
    "            if detected_hypothesis and is_similar(h['label'], detected_hypothesis['label'], max_distance=1):\n",
    "                correct_labels_count += 1\n",
    "\n",
    "    return TP, FP, FN, correct_labels_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_similar(str1, str2, max_distance=1):\n",
    "    # Check if detected is a substring of true\n",
    "    if str1 in str2:\n",
    "        return True\n",
    "\n",
    "    # If not a substring, check Levenshtein distance\n",
    "    return lev.distance(str1, str2) <= max_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(TP, FP, FN):\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_label == ground truth label\n",
    "# ex_label == extracted label\n",
    "def calculate_label_metrics(correct_hypotheses_with_labels):\n",
    "    TP = sum(is_similar(gt_label, ex_label, max_distance=1) for gt_label, ex_label in correct_hypotheses_with_labels if gt_label)\n",
    "    \n",
    "    FP_mismatched_label = sum(not is_similar(gt_label, ex_label, max_distance=1) for gt_label, ex_label in correct_hypotheses_with_labels if gt_label and ex_label)\n",
    "    FP_no_ground_truth_label = sum(1 for gt_label, ex_label in correct_hypotheses_with_labels if not gt_label and ex_label)\n",
    "    FP = FP_mismatched_label + FP_no_ground_truth_label\n",
    "    \n",
    "    FN = sum(1 for gt_label, ex_label in correct_hypotheses_with_labels if gt_label and not ex_label)\n",
    "\n",
    "    return calculate_metrics(TP, FP, FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YAML files\n",
    "\"\"\"in the output: V1, V2, V3, ...\"\"\"\n",
    "extracted_files_sets = [ \n",
    "    sorted(glob.glob('../chatGPT_short/*.yaml')),\n",
    "    sorted(glob.glob('../chatGPT_long/*.yaml')),\n",
    "    sorted(glob.glob('../chatGPT_YAML_JSON/*.yaml')),\n",
    "    sorted(glob.glob('../chatGPT_fewshot5_yaml/*.yaml'))\n",
    "    sorted(glob.glob('../chatGPT_fewshot5_yaml_JSON/*.yaml'))\n",
    "]\n",
    "ground_truth_files = sorted(glob.glob('../true_results/*.yaml'))\n",
    "\n",
    "# Function to extract filename without extension\n",
    "def get_filename_without_extension(file_path):\n",
    "    return os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "# Function to find matching ground truth file\n",
    "def find_matching_gt_file(extracted_file, ground_truth_files):\n",
    "    extracted_filename = get_filename_without_extension(extracted_file)\n",
    "    for gt_file in ground_truth_files:\n",
    "        if get_filename_without_extension(gt_file) == extracted_filename:\n",
    "            return gt_file\n",
    "    return None\n",
    "\n",
    "# Function compare extracted files to the ground truth files\n",
    "def process_extracted_files(extracted_files, ground_truth_files):\n",
    "    constructs_TP = constructs_FP = constructs_FN = 0\n",
    "    hypotheses_TP = hypotheses_FP = hypotheses_FN = 0\n",
    "    correct_labels_count = 0\n",
    "    correct_hypotheses_with_labels = []\n",
    "\n",
    "    for ex_file in extracted_files:\n",
    "        gt_file = find_matching_gt_file(ex_file, ground_truth_files)\n",
    "        if not gt_file:\n",
    "            print(f\"No matching ground truth file for {ex_file}\")\n",
    "            continue\n",
    "\n",
    "        ground_truth = load_yaml(gt_file)\n",
    "        extracted_data = load_yaml(ex_file)\n",
    "\n",
    "        # Check for constructs in ground truth and extracted data\n",
    "        if 'constructs' in ground_truth and 'constructs' in extracted_data:\n",
    "            true_constructs = ground_truth['constructs']\n",
    "            detected_constructs = extracted_data['constructs']\n",
    "            TP, FP, FN = compare_constructs(true_constructs, detected_constructs)\n",
    "            constructs_TP += TP\n",
    "            constructs_FP += FP\n",
    "            constructs_FN += FN\n",
    "\n",
    "        # Check for hypotheses in ground truth and extracted data\n",
    "        if 'hypotheses' in ground_truth and 'hypotheses' in extracted_data:\n",
    "            true_hypotheses = ground_truth['hypotheses']\n",
    "            detected_hypotheses = extracted_data['hypotheses']\n",
    "            TP, FP, FN, labels_count = compare_hypotheses(true_constructs, detected_constructs, true_hypotheses, detected_hypotheses)\n",
    "            hypotheses_TP += TP\n",
    "            hypotheses_FP += FP\n",
    "            hypotheses_FN += FN\n",
    "            correct_labels_count += labels_count\n",
    "\n",
    "            for hypothesis_id in true_hypotheses:\n",
    "                if hypothesis_id in detected_hypotheses:\n",
    "                    gt_label = true_hypotheses[hypothesis_id]['label']\n",
    "                    ex_label = detected_hypotheses[hypothesis_id]['label']\n",
    "                    correct_hypotheses_with_labels.append((gt_label, ex_label))\n",
    "\n",
    "    return constructs_TP, constructs_FP, constructs_FN, hypotheses_TP, hypotheses_FP, hypotheses_FN, correct_labels_count, correct_hypotheses_with_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructs v1 - Precision: 0.89, Recall: 0.85, F1 Score: 0.87\n",
      "Links      v1 - Precision: 0.56, Recall: 0.49, F1 Score: 0.52\n",
      "Labels     v1 - Precision: 0.74, Recall: 0.96, F1 Score: 0.84\n",
      "\n",
      "Constructs v2 - Precision: 0.85, Recall: 0.81, F1 Score: 0.83\n",
      "Links      v2 - Precision: 0.55, Recall: 0.51, F1 Score: 0.53\n",
      "Labels     v2 - Precision: 0.66, Recall: 1.00, F1 Score: 0.80\n",
      "\n",
      "Constructs v3 - Precision: 0.89, Recall: 0.90, F1 Score: 0.90\n",
      "Links      v3 - Precision: 0.72, Recall: 0.65, F1 Score: 0.68\n",
      "Labels     v3 - Precision: 0.70, Recall: 0.96, F1 Score: 0.81\n",
      "\n",
      "Constructs v4 - Precision: 0.89, Recall: 0.97, F1 Score: 0.93\n",
      "Links      v4 - Precision: 0.67, Recall: 0.60, F1 Score: 0.63\n",
      "Labels     v4 - Precision: 0.77, Recall: 0.99, F1 Score: 0.87\n",
      "\n",
      "\n",
      "Constructs gt - Precision: 0.88, Recall: 0.80, F1 Score: 0.82   (Mammoth pipeline)\n",
      "Labels     gt - Accuracy:  0.72\n"
     ]
    }
   ],
   "source": [
    "# Process each set of extracted files\n",
    "for idx, extracted_files in enumerate(extracted_files_sets):\n",
    "    constructs_TP, constructs_FP, constructs_FN, hypotheses_TP, hypotheses_FP, hypotheses_FN, correct_labels_count, correct_hypotheses_with_labels = process_extracted_files(extracted_files, ground_truth_files)\n",
    "    \n",
    "    # Calculate and print metrics for constructs\n",
    "    constructs_precision, constructs_recall, constructs_f1 = calculate_metrics(constructs_TP, constructs_FP, constructs_FN)\n",
    "    print(f\"Constructs v{idx+1} - Precision: {constructs_precision:.2f}, Recall: {constructs_recall:.2f}, F1 Score: {constructs_f1:.2f}\")\n",
    "\n",
    "    # Calculate and print metrics for hypotheses\n",
    "    hypotheses_precision, hypotheses_recall, hypotheses_f1 = calculate_metrics(hypotheses_TP, hypotheses_FP, hypotheses_FN)\n",
    "    print(f\"Links      v{idx+1} - Precision: {hypotheses_precision:.2f}, Recall: {hypotheses_recall:.2f}, F1 Score: {hypotheses_f1:.2f}\")\n",
    "\n",
    "    # Calculate and print metrics for label accuracy\n",
    "    label_precision, label_recall, label_f1 = calculate_label_metrics(correct_hypotheses_with_labels)\n",
    "    print(f\"Labels     v{idx+1} - Precision: {label_precision:.2f}, Recall: {label_recall:.2f}, F1 Score: {label_f1:.2f}\\n\")\n",
    "\n",
    "print(\"\\nConstructs gt - Precision: 0.88, Recall: 0.80, F1 Score: 0.82   (Mammoth pipeline)\")\n",
    "print(\"Labels     gt - Accuracy:  0.72\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
