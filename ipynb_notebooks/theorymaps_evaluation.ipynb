{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Python311\\Lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Python311\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "c:\\Python311\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "import random\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# Include Source path to path\n",
    "src_path = os.path.abspath(os.path.join('..'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    \n",
    "from theorymaps import ResearchModel, ResearchModelList, Node, Link, Style\n",
    "from theorymaps.metrics import comparision_list_report, comparison_report\n",
    "\n",
    "from theorymaps.generation import create_varible_list, load_unique_variables, random_list_of_variables, generate_research_model, generate_and_save_research_model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def natural_sort(l): \n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    return sorted(l, key=alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results():\n",
    "    \"\"\"\n",
    "    - iterate over both folder\n",
    "    - for each folder: add all yaml files to research model list\n",
    "    - lastly, compare both research model lists\n",
    "    \"\"\"\n",
    "    path_to_ground_truth_yamls = \"../true_results/\"\n",
    "    path_to_predicted_yamls = \"../chatGPT_long\"\n",
    "    \n",
    "    list_ground_truth = ResearchModelList()\n",
    "    list_predicted = ResearchModelList()\n",
    "    \n",
    "    # read all ground truth yamls to research models\n",
    "    #rml_id = 1\n",
    "\n",
    "    # read all predicted yamls to research models\n",
    "    #for filename in natural_sort(list(glob.iglob(f'{path_to_predicted_yamls}/*'))):\n",
    "    for full_path in natural_sort(list(glob.iglob(f'{path_to_predicted_yamls}/*.yaml'))):\n",
    "        filename = os.path.basename(full_path)\n",
    "        print(full_path)\n",
    "        model = ResearchModel()\n",
    "        model.read_yaml(full_path)\n",
    "        list_predicted.add_model(model)\n",
    "        for full_path_truth in natural_sort(list(glob.iglob(f'{path_to_ground_truth_yamls}/*'))):\n",
    "            filename_truth = os.path.basename(full_path_truth)\n",
    "            #print(filename_truth)\n",
    "            if filename_truth == filename:\n",
    "                print(full_path_truth)\n",
    "                model_truth = ResearchModel()\n",
    "                model_truth.read_yaml(full_path_truth)\n",
    "                list_ground_truth.add_model(full_path_truth)\n",
    "\n",
    "    # print length\n",
    "    #print(\"length of ground truth: \", len(list_ground_truth))\n",
    "    #print(\"length of predicted: \", len(list_predicted))\n",
    "    \n",
    "    \n",
    "    # what exactly is compared? Text is wrongly ocr'ed in diagram1. Ist this the reason for issues?\n",
    "    comparision_list_report(list_ground_truth, list_predicted, levenshtein=3)\n",
    "    \n",
    "    #list_ground_truth.to_yaml(\"01_manual_evaluation_ground_truth.yaml\")\n",
    "    #list_predicted.to_yaml(\"02_manual_evaluation_system_output.yaml\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../chatGPT_long\\diagram1.yaml\n",
      "../true_results\\diagram1.yaml\n",
      "../chatGPT_long\\diagram2.yaml\n",
      "../true_results\\diagram2.yaml\n",
      "../chatGPT_long\\diagram3.yaml\n",
      "../true_results\\diagram3.yaml\n",
      "../chatGPT_long\\diagram5.yaml\n",
      "../true_results\\diagram5.yaml\n",
      "../chatGPT_long\\diagram6.yaml\n",
      "../true_results\\diagram6.yaml\n",
      "../chatGPT_long\\diagram7.yaml\n",
      "../true_results\\diagram7.yaml\n",
      "../chatGPT_long\\diagram8.yaml\n",
      "../true_results\\diagram8.yaml\n",
      "../chatGPT_long\\diagram13.yaml\n",
      "../true_results\\diagram13.yaml\n",
      "../chatGPT_long\\diagram18.yaml\n",
      "../true_results\\diagram18.yaml\n",
      "../chatGPT_long\\diagram21.yaml\n",
      "../true_results\\diagram21.yaml\n",
      "../chatGPT_long\\diagram26.yaml\n",
      "../true_results\\diagram26.yaml\n",
      "../chatGPT_long\\diagram28.yaml\n",
      "../true_results\\diagram28.yaml\n",
      "../chatGPT_long\\diagram30.yaml\n",
      "../true_results\\diagram30.yaml\n",
      "../chatGPT_long\\diagram35.yaml\n",
      "../true_results\\diagram35.yaml\n",
      "../chatGPT_long\\diagram37.yaml\n",
      "../true_results\\diagram37.yaml\n",
      "../chatGPT_long\\diagram42.yaml\n",
      "../true_results\\diagram42.yaml\n",
      "../chatGPT_long\\diagram45.yaml\n",
      "../true_results\\diagram45.yaml\n",
      "../chatGPT_long\\diagram48.yaml\n",
      "../true_results\\diagram48.yaml\n",
      "../chatGPT_long\\diagram50.yaml\n",
      "../true_results\\diagram50.yaml\n",
      "../chatGPT_long\\diagram58.yaml\n",
      "../true_results\\diagram58.yaml\n",
      "../chatGPT_long\\diagram60.yaml\n",
      "../true_results\\diagram60.yaml\n",
      "../chatGPT_long\\diagram63.yaml\n",
      "../true_results\\diagram63.yaml\n",
      "../chatGPT_long\\diagram66.yaml\n",
      "../true_results\\diagram66.yaml\n",
      "../chatGPT_long\\diagram71.yaml\n",
      "../true_results\\diagram71.yaml\n",
      "../chatGPT_long\\diagram75.yaml\n",
      "../true_results\\diagram75.yaml\n",
      "../chatGPT_long\\diagram84.yaml\n",
      "../true_results\\diagram84.yaml\n",
      "../chatGPT_long\\diagram87.yaml\n",
      "../true_results\\diagram87.yaml\n",
      "../chatGPT_long\\diagram99.yaml\n",
      "../true_results\\diagram99.yaml\n",
      "../chatGPT_long\\diagram100.yaml\n",
      "../true_results\\diagram100.yaml\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'nodes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcompare_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 39\u001b[0m, in \u001b[0;36mcompare_results\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m             list_ground_truth\u001b[38;5;241m.\u001b[39madd_model(full_path_truth)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# print length\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#print(\"length of ground truth: \", len(list_ground_truth))\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#print(\"length of predicted: \", len(list_predicted))\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# what exactly is compared? Text is wrongly ocr'ed in diagram1. Ist this the reason for issues?\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[43mcomparision_list_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_ground_truth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_predicted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevenshtein\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m#list_ground_truth.to_yaml(\"01_manual_evaluation_ground_truth.yaml\")\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m#list_predicted.to_yaml(\"02_manual_evaluation_system_output.yaml\")\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\theorymaps\\metrics.py:454\u001b[0m, in \u001b[0;36mcomparision_list_report\u001b[1;34m(true_model_list, predicted_model_list, levenshtein)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcomparision_list_report\u001b[39m(\n\u001b[0;32m    450\u001b[0m     true_model_list: ResearchModelList,\n\u001b[0;32m    451\u001b[0m     predicted_model_list: ResearchModelList,\n\u001b[0;32m    452\u001b[0m     levenshtein\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    453\u001b[0m ):\n\u001b[1;32m--> 454\u001b[0m     node_results \u001b[38;5;241m=\u001b[39m \u001b[43mcompare_lists\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrue_model_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicted_model_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomparision_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompare_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevenshtein\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevenshtein\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m     node_errors \u001b[38;5;241m=\u001b[39m node_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m node_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfn\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    462\u001b[0m     number_of_nodes \u001b[38;5;241m=\u001b[39m node_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_instances_true_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\theorymaps\\metrics.py:415\u001b[0m, in \u001b[0;36mcompare_lists\u001b[1;34m(true_model_list, predicted_model_list, comparision_method, levenshtein)\u001b[0m\n\u001b[0;32m    413\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m true_model, predicted_model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(true_model_list, predicted_model_list):\n\u001b[1;32m--> 415\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mcomparision_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevenshtein\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m all_metrics:\n\u001b[0;32m    417\u001b[0m         sum_results[metric] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m results[metric]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\theorymaps\\metrics.py:251\u001b[0m, in \u001b[0;36mcompare_nodes\u001b[1;34m(true_model, predicted_model, levenshtein)\u001b[0m\n\u001b[0;32m    249\u001b[0m nearly_correct_instances \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    250\u001b[0m false_positive_instances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(predicted_model\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m--> 251\u001b[0m false_negative_instances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mtrue_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m predicted_node \u001b[38;5;129;01min\u001b[39;00m predicted_model\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m true_node \u001b[38;5;129;01min\u001b[39;00m false_negative_instances:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'nodes'"
     ]
    }
   ],
   "source": [
    "compare_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
